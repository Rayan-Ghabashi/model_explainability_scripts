# Machine Learning Model Explainability

This project explores three methods to explain machine learning models:

## Methods

### 1. Permutation Importance

Permutation Importance measures the effect of shuffling a feature on the model’s performance. It helps identify which features are most important for predictions.

### 2. Partial Dependence Plots (PDP)

Partial Dependence Plots visualize the relationship between a feature and the predicted outcome, showing how predictions change as the feature value changes.

### 3. SHAP Values

SHAP (SHapley Additive exPlanations) values provide a unified measure of feature importance. They allocate credit for a model’s prediction to each feature, considering all possible feature combinations.

## Source

For more detailed tutorials, visit the [Kaggle Machine Learning Explainability course](https://www.kaggle.com/learn/machine-learning-explainability).
